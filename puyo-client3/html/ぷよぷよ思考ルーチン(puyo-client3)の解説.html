<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type">
    <title>ぷよぷよ思考ルーチン(puyo-client3)の解説</title>
  </head>
  <body>
    <h1>ぷよぷよ思考ルーチン(puyo-client3)の解説</h1>
    <p>サンプルプログラムとして用意したpuyo-client3の思考ルーチンについて解説する。</p>
    <p><br>
    </p>
    <h2>基本的な考え方</h2>
    <p>現在のぷよと次のぷよの2手について、全ての置ける位置、回転のパターンを列挙し、それぞれ置いた結果について評価を行い、最も評価の点数が高い位置に置くようにする。</p>
    局面の評価は、以下のような観点で評価で行う。<br>
    <ol>
      <li>連鎖が発生するか</li>
      <li>積まれた高さ</li>
      <li>ぷよが連結しているか</li>
      <li>下方向に異なる色を挟んで同色があるか</li>
      <li>下方向の異なる色のぷよが消えた場合に隣の列と連結するか</li>
      <li>発火ポイント(隣に空白があるぷよが消えた場合に連鎖が発生するか)</li>
      <li>高さが一定以上の場合発火を優先する</li>
    </ol>
    <p>それぞれの評価観点について、重み付けを行い、加算した値を評価値とする。</p>
    <p>重みは、経験則や勘に頼り適当に決めて、実際に対戦させて誤った位置に置いているようであれば修正を行う。</p>
    <br>
    <h2>機械学習</h2>
    <p>上記の思考ルーチンの作り方は、評価関数を経験に基づき決めたうえで、重みを勘に頼って適当に決めるというヒューリスティックな方法を採用している。</p>
    <p>ここでは、重みを実際のプレイヤーが対戦した結果(ぷよ譜)から学習して決めることを試みる。（なお、機械学習では重みのことを特徴量、全ての重みをまとめたものを特徴ベクトルと言う。）</p>
    <p>学習の方法は以下のようにして行う。</p>
    <ol>
      <li>重みに適当な初期値を与える。</li>
      <li>ぷよ譜の局面について思考ルーチンで置く位置を求める。</li>
      <li>思考ルーチンで求めた位置とぷよ譜で置かれた位置が一致していれば一致度を+1する。</li>
      <li>用意したぷよ譜にすべてに対して、一致度を測る。(重みに対して一致度を測る関数を目的関数と呼ぶ)</li>
      <li>1つ重みについて、値を少し変化させる。</li>
      <li>同様に一致度を測り、一致度が高くなる場合、その値に重みを更新する。</li>
      <li>すべての重みについて、同様に一致度が高くなるように値を変化させていく。</li>
    </ol>
    <p>基本的な考え方は以上であるが、これを実際行おうとするとうまくいかない。</p>
    <p>なぜなら、以下のような状況があるためである。</p>
    <ol>
      <li>値を少し変化させたときに一致度が変化しない場合がある。</li>
      <li>一致度が低くなるためそれ以上変化しなくなる場合がある。</li>
    </ol>
    <p>前者は、目的関数が微分不可能なために発生する。後者は、変化しなくなった値が正解(最適解)であればよいが、そうでない場合は局所解を求めていることになる。</p>
    <p>逆にいうと、目的関数が微分可能であり、初期値が最適解の近くにないとうまくいかない。</p>
    値を繰り返し変化させて最適解を求める方法を一般的に勾配法と呼び、最急降下法や共役勾配法などが知られているが、必ず最適解を見つけられる万能な手段というのは知られていない。
    <p><br>
    </p>
    <p>サンプルでは、値を少し変化させるのではなく、あらかじめ決めた値の範囲を一定間隔で変化させて一致度が最も高くなる値に更新するという方法をとることで、改善を図っている。</p>
    <p>各重みの軸方向で最も高い山を探し、その山の頂上から別の重みの軸方向を眺めて最も高い山を探していくイメージである。</p>
    <p>これを繰り返すことで、最適解に近づけていく。</p>
    <p>この方法でも、局所解に陥ることは避けられない。そのため、調べる値の範囲と初期値については、まずは変化させる間隔を荒くして、おおざっぱに傾向を調べてから決める必要がある。</p>
    <p>この方法では、重みの変化を毎回決めた範囲すべてについて行うため、計算時間はより多くかかる。</p>
    <p>重みの種類が多い場合はこの方法では実用的ではなくなるが、サンプルでは重みの種類がそれほど多くないため、この方法を採用した。</p>
    <p>それでも、15手目までのぷよ譜200個のすべての局面について、22種類の重みそれぞれについて変化させる回数を10くらいにして、学習させた場合、1回の更新に数10分を要した。</p>
    <p>これ以上、学習させるぷよ譜を増やしたり、重みの種類を増やすと計算に1日以上かかるようなことになるので、学習時間を短くするための工夫が必要になる。</p>
    <p>なお、2006年世界コンピュータ将棋選手権で優勝した将棋ソフトのBonanzaでは、要素数が5千万以上の特徴ベクトルをプロの棋譜から機械学習させており、目的関数や勾配法には効率よく計算できて局所解に陥らないようにするための工夫がされている。<br>
      (Bonanzaの思考ルーチンについては、インターネット上や書籍で解説があるので、興味があれば読んでみるとよいと思う。)</p>
    <p><br>
    </p>
    <p>サンプルで、ぷよ譜を学習させて求めた重みでの一致度は3000局面に対して500程度であり、精度はあまり高くならなかった。</p>
    <p>これは、以下のような理由が考えられる。</p>
    <ol>
      <li>サンプルの評価関数で採用している観点とは、別の観点でぷよ譜のプレイヤーが置く位置を決めている。</li>
      <li>いろんなプレイヤーのぷよ譜を使用しているため、各プレイヤーごとでプレイスタイルが異なっている。</li>
      <li>ぷよ譜はインターネット上にあった強豪プレイヤーの対戦結果を用いているが、ぷよぷよはリアルタイム制の対戦ゲームであるため、プレイヤーが毎回同じように置けているとは限らない。</li>
    </ol>
    <p>学習データを単一プレイヤーのものにしてみるなど、仮説をそれぞれ検証してみることで、より強い思考ルーチンが作れるものと思われる。</p>
    <p>サンプルでは、そこまでの検証は行わなかった。</p>
    <p><br>
    </p>
    <h3>補足1</h3>
    <p>このように学習データに正解が含まれるデータで機械学習を行う方法は、教師あり学習と呼ばれている。<br>
      機械学習には、強化学習や、教師なし学習といった方法もある。最近話題のディープラーニングは教師なし学習である。</p>
    <h3>補足2</h3>
    <p>サンプルではぷよを消したときの連鎖の評価の重みは固定値として、それとの相対的な重みを求めている。<br>
      すべての重みを可変とした場合は、整数倍の値も最適解になってしまう。<br>
      これを防ぐ一般的な方法として、ラグランジュ未定乗数法で重みに制約条件を与える方法がある。</p>
    <p>また、学習データの中に誤りのデータが含まれていた場合に誤りまで学習してしまわないようにするためや、未知の局面に対しても安定した結果を返せるようにするために、目的関数に正則化項(L1正則化もしくはL2正則化)を加えることがある。(このような問題のことを過学習と呼ぶ。)</p>
    <h3>ソースと学習データについて<br>
    </h3>
    <p>機械学習のソースは、puyo-client3\src-leanにある。<br>
      使用した200個の機械学習用のぷよ譜データは、puyo-client3\puyofuにある。<br>
      インターネット上にあった強豪プレイヤーのぷよ譜データのうち一部を使用している。<br>
      サンプル使用しなかったデータも含めて2802件分のデータはpuyofudata\outにある。<br>
      機械学習で求めた重みの値は、直接思考ルーチンのソース（puyo-client3\src\puyo\client3\PuyoPuyoAIImpl.java）に反映している。</p>
    <p><br>
    </p>
    <h2>改善点</h2>
    <p>サンプルは、手を抜いて作成しており、まだまだ改善余地を残している。</p>
    <p>さらなる改善のためには、以下のようなことが検討できる。</p>
    <ol>
      <li>サンプルでは簡単にするため、相手の状況は評価しないが、相手の状況も考慮にいれる。</li>
      <li>評価観点をさらに追加する。</li>
      <li>ぷよ譜から序盤の定石を作成する。</li>
      <li>サンプルでは高速化のためのチューニングは行っていないが、高速化することで機械学習の計算時間を短縮できる。</li>
      <li>サンプルでは2手先までしか読んでいないが、3手先以降も考慮する。</li>
    </ol>
    <p>なお、ぷよぷよでは、3手先以降のぷよが不定のため、3手先以上を考慮するには、工夫が必要になる。</p>
    <p>ランダムにぷよを生成して確率的に求める方法(モンテカルロ法)などがあるが、ランダムに生成したものから外れた場合はかえって弱くなる。</p>
    <p>読む手を増やすとランダムのパターンの組み合わせが多くなり、思考時間が指数関数的に増加するなど、3手以上読む思考ルーチンの作成は難しいが、インターネット上に効果があるという論文も見つかった。</p>
    <p>他にもいろいろ改善方法はあると思うので試してもらいたい。</p>
    <p>サンプルでは学習結果の評価をきちんと行わなかったが、交差確認法などで学習結果を評価することで精度を客観的に評価できる。</p>
    <p>機械学習ははじめ難しそうに思えるかもしれないが、ここで解説しているような方法であればそれほど難しいことではないので、実際ソースコードを書いて行ってみると理解が深まりより身近に感じられるものになると思う。</p>
    <p><br>
    </p>
    <h2>参考文献</h2>
    <ol>
      <li>コンピュータ将棋の進歩 6 -プロ棋士に並ぶ- →Bonanzaの機械学習について解説がある。</li>
      <li>フリーソフトではじめる機械学習入門 →機械学習の手法の概要を知るにはよい。詳しく知るにはもっと詳しい専門書にあたる必要がある。</li>
    </ol>
  </body>
</html>
